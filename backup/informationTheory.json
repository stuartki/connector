[{"description": "H(X) = -sum(x, X)[p(x)log(p(x))] is called the entropy of a discrete random vvariable ", "title": "H(X) entropy", "related": [], "past": [], "future": [2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 34, 37, 39], "keywords": [], "type": ["definition", "end"], "id": 1}, {"description": "The entropy of X can also be interpreted as the expected value of log(1/p(x)) or E[log(1/p(X))] ", "title": "expected value of log(1/p(x)) is entropy", "related": [], "past": [1], "future": [5], "keywords": [], "type": ["implication", "H(X)", "end"], "id": 2}, {"description": "H(X) >= 0. 0 <= p(x) <= 1 implies log(1/p(x)) >= 0 ", "title": "H(X) >= 0", "related": [], "past": [1], "future": [], "keywords": [], "type": ["Lemma 2.1.1", "end"], "id": 3}, {"description": "H(X) = (logb(a))Ha(X). different bases can be converted ", "title": "different base of H(X) can be converted", "related": [], "past": [1], "future": [], "keywords": [], "type": ["Lemma 2.1.2", "end"], "id": 4}, {"description": "joint entropy H(X,Y) of a pair of discrete random variables (X,Y) with a joint distribution p(x,y) is defined as H(X,Y) = -sum(x)sum(y)[p(x,y)log(p(x,y))] or the -Elog(p(X,Y)) ", "title": "joint entropy H(X,Y)", "related": [], "past": [1, 2], "future": [6, 7, 8, 11, 26, 56], "keywords": [], "type": ["definition", "end"], "id": 5}, {"description": "If (X,Y) ~ p(x,y) then the conditional entropy H(Y|X) is defined as sum(x)[p(x)H(Y|X = x) or E(p(x,y))[-log(p(Y|X))], the expected value of the entropies of the conditional distributionals ", "title": "conditional entropy H(Y|X)", "related": [], "past": [5, 1], "future": [7, 8, 11, 12, 13, 16, 34, 57, 59], "keywords": [], "type": ["definition", "end"], "id": 6}, {"description": "H(X,Y) = H(X) + H(Y|X) is the chain rule. It can be interpreted as log(p(X,Y)) = log(p(X)) + log(p(Y|X)), and take expectation of both sides.", "title": "chain rule H(X,Y)", "related": [], "past": [5, 6, 1], "future": [13], "keywords": [], "type": ["Theorem 2.2.1", "end"], "id": 7}, {"description": "H(X,Y|Z) = H(X|Z) + H(Y|X,Z) ", "title": "H(X,Y|Z) = H(X|Z) + H(Y|X,Z) conditional joint entropy", "related": [], "past": [5, 6], "future": [16], "keywords": [], "type": ["Corollary", "end"], "id": 8}, {"description": "relative entropy between two probability mass functions p(x) and q(x) is defined as D(p||q) = sum(x)[p(x)log(p(x)/q(x)) = Eplog(p(X)/q(X)). It is the measure of the distance between two distributions. It arises as an expected logarithm of the likelihood ratio. It is a measure of the inefficiency of assuming that the distribution is q when the true distribution is p.  ", "title": "relative entropy", "related": [], "past": [1], "future": [10, 16, 17, 22], "keywords": [], "type": ["definition", "end"], "id": 9}, {"description": "Consider two random variables X and Y with a joint probability mass function p(x,y) and the marginal probability mass functions. The mutual information I(X;Y) is the relative entropy between the joint distribution and the product distribbution p(x)p(y). I(X;Y) = sum(x, y)[p(x,y)log(p(x,y)/p(x)p(y)) = D(p(x,y)||p(x)p(y)) = Ep(x,y)[log(p(X,Y)/p(X)p(Y)] ", "title": "mutual information I(X;Y)", "related": [], "past": [9], "future": [11, 12, 14, 15], "keywords": [], "type": ["definition", "end"], "id": 10}, {"description": "I(X;Y) = H(X) - H(X|Y) or the mutual information is the reduction in uncertainnty of X due to knowledge of Y. H(Y) - H(Y|X) = I(X;Y) = H(X) + H(Y) - H(X,Y) ", "title": "I(X;Y) = H(X) - H(X|Y) reduction of uncertainty is mutual information", "related": [], "past": [10, 1, 5, 6], "future": [12, 14, 15], "keywords": [], "type": ["implication", "end"], "id": 11}, {"description": "I(X;Y) = I(Y;X) and I(X;X) = H(X) ", "title": "properties of mutual information and entropy", "related": [], "past": [1, 6, 10, 11], "future": [], "keywords": [], "type": ["Theorem 2.4.1", "end"], "id": 12}, {"description": "chain rule for entropy is if X1, X2, ... , Xn be drawn according to p(x1,x2,...,xn) then H(X1, X2, ... , Xn) = sum(n)[H(Xi|Hi-1,...,X1)] ", "title": "chain rule for entropy", "related": [], "past": [7, 6, 1], "future": [26], "keywords": [], "type": ["Theorem 2.5.1", "end"], "id": 13}, {"description": "conditional mutual information of random variables is I(X;Y|Z) = H(X|Z) - H(X|Y,Z). I(X;Y|Z) = H(X|Z) - H(X|Y,Z) = Ep(x,y,z)log(p(X,Y|Z)/p(X|Z)p(Y|Z)) ", "title": "conditional mutual information", "related": [], "past": [10, 11], "future": [15, 16], "keywords": [], "type": ["definition", "end"], "id": 14}, {"description": "chain rule for information is I(X1,X2,...,Xn;Y) = sum(n)[I(Xi;Y|Xi-1,...X1)] ", "title": "chain rule for information", "related": [], "past": [10, 14, 11], "future": [31], "keywords": [], "type": ["Theorem 2.5.2", "end"], "id": 15}, {"description": "conditional relative entropy D(p(y|x)||q(y|x)) ", "title": "conditional relative entropy", "related": [], "past": [9, 6, 14, 8], "future": [17], "keywords": [], "type": [], "id": 16}, {"description": "chain rule for relative entropy ", "title": "chain rule for relative entropy", "related": [], "past": [9, 16], "future": [], "keywords": [], "type": ["relative", "Theorem 2.5.3"], "id": 17}, {"description": "function f(x) is said to be convex over an interval (a,b) if for every x1, x2 in (a,b) and 0 <= l <= 1, f(lx1 + (1-l)x2) <= lf(x1) + (1-l)f(x2). It is said to be strictly convex if equality holds only if l = 0 or l = 1 ", "title": "convex function", "related": [], "past": [], "future": [19, 20, 21], "keywords": [], "type": ["definition"], "id": 18}, {"description": "function is concave if -f is convex ", "title": "concave function", "related": [], "past": [18], "future": [], "keywords": [], "type": ["definition"], "id": 19}, {"description": "If the function f has a second derivative that is non-negative over an interval, the function is convex (strictly convex) over that interval ", "title": "second derivative is non-negative means convex", "related": [], "past": [18], "future": [], "keywords": [], "type": ["Theorem 2.6.1"], "id": 20}, {"description": "Jensen's inequality states that if f is a convex function and X is a random variable Ef(X) >= f(EX). Moreover, if f is strictly convex, the equality implies that X = EX with probability 1 or when X is a constant. ", "title": "Jensen's inequality", "related": [], "past": [18], "future": [22], "keywords": [], "type": ["Theorem 2.6.2"], "id": 21}, {"description": "information inequality states that for D(p||q) >= 0 with equality if and only if p(x) = q(x) for all x ", "title": "information inequality D(p||q) >= 0", "related": [], "past": [21, 9], "future": [23, 24], "keywords": [], "type": ["Theorem 2.6.3"], "id": 22}, {"description": "nonnegativity of mutual information I(X;Y) >= 0 with equality if and only if X and Y are independent ", "title": "nonnegativity of mutual information I(X;Y) >= 0", "related": [], "past": [22], "future": [25], "keywords": [], "type": ["Corollary"], "id": 23}, {"description": "H(X) <= log|X| where |X| denotes the number of elements in the range of X, with equality if and only if X has a uniform distribution over X.  ", "title": "H(X) <= log|X|", "related": [], "past": [22], "future": [], "keywords": [], "type": ["Theorem 2.6.4"], "id": 24}, {"description": "Conditioning reduces entropy H(X|Y) <= H(X) with equality if and only if X and Y are independent ", "title": "conditioning reduces entropy H(X|Y) <= H(X)", "related": [], "past": [23], "future": [26, 59], "keywords": [], "type": ["Theorem 2.6.5"], "id": 25}, {"description": "independence bound on entropy. Let X1,X2,...Xn be drawn according to p(x1,x2,...,xn) then H(X1,X2,...,Xn) <= sum(n)[H(Xi)] with equality if and only if Xi are independent ", "title": "independence bound on entropy H(X1,X2,...,Xn) <= sum(n)[H(Xi)]", "related": [], "past": [5, 25, 13], "future": [], "keywords": [], "type": ["Theorem 2.6.6"], "id": 26}, {"description": "random variables form a Markov chain in that order (X -> Y -> Z) if the conditional distribution depends only on Y and is conditionally indpendent of X. p(x,y,z) = p(x)p(y|x)p(z|y) ", "title": "Markov Chain X->Y->Z", "related": [], "past": [], "future": [28, 29, 30, 33, 34, 46], "keywords": [], "type": ["definition"], "id": 27}, {"description": "X->Y->Z iff X and Z are conditionally indpendent given Y. Markovity implies p(x,z|y) = p(x|y)p(z|y) ", "title": "Markovity implies p(x,z|y) = p(x|y)p(z|y)", "related": [], "past": [27], "future": [31], "keywords": [], "type": ["implication"], "id": 28}, {"description": "X->Y->Z implies Z->Y->X ", "title": "X->Y->Z implies Z->Y->X", "related": [], "past": [27], "future": [], "keywords": [], "type": ["implication"], "id": 29}, {"description": "if Z = f(Y), then X->Y->Z ", "title": "if Z = f(Y), then X->Y->Z", "related": [], "past": [27], "future": [], "keywords": [], "type": ["implication"], "id": 30}, {"description": "if X-Y->Z, then I(X;Y) >= I(X;Z) ", "title": "data-processing inequality", "related": [], "past": [15, 28], "future": [32, 34], "keywords": [], "type": ["Theorem 2.8.1"], "id": 31}, {"description": "if Z = g(Y), we have I(X;Y) >= I(X;g(Y)) ", "title": "if Z = g(Y), we have I(X;Y) >= I(X;g(Y))", "related": [], "past": [31], "future": [], "keywords": [], "type": ["corollary"], "id": 32}, {"description": "for X->Y->X^ is a Markov chain, the probability of error is Pe = Pr{X^ != X} ", "title": "probability of error", "related": [], "past": [27], "future": [34], "keywords": [], "type": ["definition"], "id": 33}, {"description": "For any estimator X^ such that X->Y->X^, with Pe, we have H(Pe) + Pe(log|X|) >= H(X|X^) >= H(X|Y). It can be weakened to 1 + Pelog|X| >= H(X|Y)  ", "title": "Fano's Inequality", "related": [], "past": [1, 6, 27, 33, 31], "future": [35], "keywords": [], "type": ["Theorem 2.10.1"], "id": 34}, {"description": "for any two random variables X and Y, let p = Pr(X!=Y). then H(p) + plog|X| >= H(X|Y) ", "title": "H(p) + plog|X| >= H(X|Y) for p = Pr(X!=Y)", "related": [], "past": [34], "future": [], "keywords": [], "type": ["corollary"], "id": 35}, {"description": "convergence of random variables is, given a sequence of random variables, X1,... we say that the sequence converges to random variable X if (in probability) if for every e > 0, Pr{|Xn-X| > e} -> 0, (in mean square) if E(Xn-X)^2 -> 0, and (with probability 1) if Pr{lim[n->inf]Xn = X} = 1 ", "title": "convergence of random variables", "related": [], "past": [], "future": [37, 38], "keywords": [], "type": ["definition"], "id": 36}, {"description": "asymptotic equipartition property is formalized by if X1,X2,...are i.i.d. ~ p(x), then -(1/n)log(p(X1,X2,...,Xn)) -> H(X) in probability ", "title": "asymptotic equipartition property AEP", "related": [], "past": [36, 1, 38, 61], "future": [41], "keywords": [], "type": ["Theorem 3.1.1"], "id": 37}, {"description": "weak law of large numbers ", "title": "weak law of large numbers", "related": [], "past": [36], "future": [37], "keywords": [], "type": ["theorem"], "id": 38}, {"description": "the typical set Ae(n) with respect to p(x) is the set of sequences (x1,x2,...,xn) in Xn with the property 2^(-n(H(X)+e)) <= p(x1,x2,...,xn) <= 2^(-n(H(X)-e)) ", "title": "typical set A", "related": [], "past": [1], "future": [40, 41, 42, 43], "keywords": [], "type": ["definition"], "id": 39}, {"description": "if (x1,x2,...,xn) in A then H(X) - e <= -1/nlog(p(x1,x2,...,xn) <= H(X) + e ", "title": "if (x1,x2,...,xn) in A then H(X) - e <= -1/nlog(p(x1,x2,...,xn) <= H(X) + e", "related": [], "past": [39], "future": [], "keywords": [], "type": ["Theorem 3.1.2a"], "id": 40}, {"description": "Pr{A} > 1 - e for n sufficiently large ", "title": "Pr{A} > 1 - e for n sufficiently large", "related": [], "past": [39, 37], "future": [43], "keywords": [], "type": ["Theorem 3.1.2b"], "id": 41}, {"description": "|A| <= 2^n(H(X)+e), where |A| denotes the number of elements in set A ", "title": "upper bound on number of elements in A", "related": [], "past": [39], "future": [], "keywords": [], "type": ["Theorem 3.1.2c"], "id": 42}, {"description": "for n sufficiently large, |A| >= (1-e)2^(n(H(X)-e)) ", "title": "lower bound on number of elements in A", "related": [], "past": [41, 39], "future": [], "keywords": [], "type": ["Theorem 3.1.2d"], "id": 43}, {"description": "a stochastic process is an indexed sequence of random variables ", "title": "stochastic process", "related": [], "past": [], "future": [45, 46, 56, 57, 58, 59], "keywords": [], "type": ["definition"], "id": 44}, {"description": "stationary if the joint distribution of any subset of the sequence of random variables is invariant with respect to shifts in the time index; that is, Pr(X1=x1, X2=x2, ... , Xn= xn) = Pr(X1+l = x1, X2+l = x2, Xn+l = xn) for every n and every shift l and for all x1,x2,...,xn in X ", "title": "stationary process", "related": [], "past": [44], "future": [54, 55, 58, 59], "keywords": [], "type": ["definition"], "id": 45}, {"description": "Markov chain is a stochastic process X1,X2,..., is said to be a Markov chain or a Markov process if for n = 1,2,... Pr(Xn+1 = xn+1|Xn=xn, Xn-1=xn-1,...,X1=x1) = Pr(Xn+1 = xn+1|Xn=xn) ", "title": "Markov chain or Markov process", "related": [], "past": [27, 44], "future": [47, 48, 49, 50, 51, 52, 54, 55], "keywords": [], "type": ["definition"], "id": 46}, {"description": "markov chain is conditionally independent given the last time process ", "title": "markov chain is conditionally independent given the last time process", "related": [], "past": [46], "future": [], "keywords": [], "type": ["implication"], "id": 47}, {"description": "Markov chain is said to be time invariant if the conditional probability p(xn+1|xn) does not depend on n; that is, for n = 1,2,..., Pr(Xn+1 = b|Xn = a) = Pr(X22 = b|X1 = a) for all a, b in X ", "title": "time invariant Markov chain", "related": [], "past": [46], "future": [50, 54], "keywords": [], "type": ["definition"], "id": 48}, {"description": "Xn is called state at time n ", "title": "state of Markov chain", "related": [], "past": [46], "future": [50, 53, 54], "keywords": [], "type": ["definition"], "id": 49}, {"description": "probability transition matrix is Pij where Pr(Xn+1 = j|Xn = i) for time invariant and initial state ", "title": "probability transition matrix", "related": [], "past": [48, 46, 49], "future": [53], "keywords": [], "type": ["definition"], "id": 50}, {"description": "irreducible if it is possible to go with positive probability from any state to another in a finite number of steps in a Markov chain ", "title": "irreducible", "related": [], "past": [46], "future": [55], "keywords": [], "type": ["definition"], "id": 51}, {"description": "if the largest common factor of the lengths of different paths from a state to itself is 1, the Markov chain is said to be aperiodic ", "title": "aperiodic", "related": [], "past": [46], "future": [55], "keywords": [], "type": ["definition"], "id": 52}, {"description": "a distribution on the states such that the distribution at time n + 1 is the same as the distribution at time n is called a stationary distribution ", "title": "stationary distribution", "related": [], "past": [50, 49], "future": [54, 55], "keywords": [], "type": ["definition"], "id": 53}, {"description": "the stationary distribution if the initial state of a Markov chain is drawn according to the stationary distribution, the Markov chain forms a stationary process ", "title": "Markov initial state is stationary, then the Markov chain forms a stationary process", "related": [], "past": [46, 49, 48, 53, 45], "future": [55], "keywords": [], "type": [], "id": 54}, {"description": "if the finite state Markov chain is irreducible and aperiodic, the stationary distribution is unique and from any starting distribution, the distribution of Xn tends to the stationary distrbution as n -> infinity ", "title": "finite state Markov chain is irreducible and aperiodic, the stationary distribution is unique and tends to it as n -> infinity", "related": [], "past": [46, 45, 53, 54, 51, 52], "future": [], "keywords": [], "type": [], "id": 55}, {"description": "entropy of a stochastic process is defined by H(X) = lim[n->inf](1/n)H(X1,X2,...,Xn) where the limit exists ", "title": "entropy rate", "related": [], "past": [44, 5], "future": [58], "keywords": [], "type": ["entropy rate"], "id": 56}, {"description": "rate of innovation is H'(X) = lim[n->inf]H(Xn|Xn-1,Xn-2,...,X1) ", "title": "rate of innovation", "related": [], "past": [44, 6, 59], "future": [58], "keywords": [], "type": ["definition"], "id": 57}, {"description": "for a stationary stochastic process, the limits entropy rate and rate of innovation are equal: H(X) = H'(X) ", "title": "stationary stochastic process, the limits entropy rate and rate of innovation are equal", "related": [], "past": [45, 44, 56, 57, 59, 60], "future": [61], "keywords": [], "type": ["Theorem 4.2.1"], "id": 58}, {"description": "H(Xn|Xn-1,...,X1) is nonincreasing in n and has a limit H'(X) for a stationary stochastic process ", "title": "H(Xn|Xn-1,...,X1) is nonincreasing in n and has a limit H'(X) for a stationary stochastic process", "related": [], "past": [6, 45, 44, 25], "future": [57, 58], "keywords": [], "type": ["Theorem 4.2.2"], "id": 59}, {"description": "cesaro mean ", "title": "cesaro mean", "related": [], "past": [], "future": [58], "keywords": [], "type": ["Theorem 4.2.3"], "id": 60}, {"description": "the general AEP for stationary ergodic processes, -1/nlogp(X1,X2,...,Xn) -> H(X) ", "title": "AEP for stationary ergodic processes", "related": [], "past": [58], "future": [37], "keywords": [], "type": ["implication"], "id": 61}]